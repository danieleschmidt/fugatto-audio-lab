#!/usr/bin/env python3
"""Enhanced Generation 2 - MAKE IT ROBUST (Reliable)

Adds comprehensive error handling, validation, monitoring, and resilience:
- Advanced error handling with recovery strategies
- Input validation and sanitization
- Health monitoring and alerting
- Graceful degradation
- Audit logging and security
- Resource management and cleanup
"""

import logging
import time
import json
import traceback
import hashlib
import threading
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from enum import Enum
from contextlib import contextmanager
import warnings
from functools import wraps

# Configure robust logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('generation2_robust.log'),
        logging.FileHandler('audit.log')  # Separate audit log
    ]
)

logger = logging.getLogger(__name__)
audit_logger = logging.getLogger('audit')

# Import core components with robust error handling
try:
    from fugatto_lab import (
        QuantumTaskPlanner, 
        QuantumTask, 
        TaskPriority,
        create_audio_generation_pipeline,
        run_quantum_audio_pipeline
    )
    from fugatto_lab.core import FugattoModel, AudioProcessor
    CORE_AVAILABLE = True
except ImportError as e:
    logger.error(f"Core import error: {e}")
    CORE_AVAILABLE = False
    
    # Robust fallback implementations
    class MockQuantumTaskPlanner:
        def __init__(self, *args, **kwargs):
            self.tasks = []
            self.max_concurrent = kwargs.get('max_concurrent_tasks', 2)
            
        def add_task(self, task):
            logger.info(f"Mock: Added task {task}")
            self.tasks.append(task)
            return f"mock_task_{len(self.tasks)}"
        
        def execute_pipeline(self, pipeline_id):
            logger.info(f"Mock: Executing pipeline {pipeline_id}")
            return {"status": "completed", "results": [], "tasks_executed": len(self.tasks)}
    
    QuantumTaskPlanner = MockQuantumTaskPlanner
    QuantumTask = dict
    TaskPriority = type("TaskPriority", (), {"HIGH": "high", "MEDIUM": "medium", "LOW": "low", "CRITICAL": "critical"})
    
    def create_audio_generation_pipeline(prompts):
        return MockQuantumTaskPlanner()
    
    def run_quantum_audio_pipeline(planner, pipeline_id=None):
        return planner.execute_pipeline(pipeline_id)


class ErrorSeverity(Enum):
    """Error severity levels for robust error handling."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class SystemHealth(Enum):
    """System health states."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    CRITICAL = "critical"
    FAILURE = "failure"


@dataclass
class ErrorContext:
    """Comprehensive error context for debugging and recovery."""
    error_type: str
    severity: ErrorSeverity
    message: str
    traceback: str
    context: Dict[str, Any]
    timestamp: float
    recovery_attempted: bool = False
    recovery_successful: bool = False
    user_impact: str = "none"


@dataclass
class HealthMetrics:
    """System health and performance metrics."""
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    error_rate: float = 0.0
    response_time: float = 0.0
    success_rate: float = 100.0
    active_tasks: int = 0
    system_health: SystemHealth = SystemHealth.HEALTHY
    alerts: List[str] = None
    
    def __post_init__(self):
        if self.alerts is None:
            self.alerts = []


class RobustErrorHandler:
    """Advanced error handling with recovery strategies and monitoring."""
    
    def __init__(self):
        self.error_history: List[ErrorContext] = []
        self.recovery_strategies: Dict[str, callable] = {
            "ImportError": self._recover_missing_dependency,
            "FileNotFoundError": self._recover_missing_file,
            "MemoryError": self._recover_memory_issue,
            "TimeoutError": self._recover_timeout,
            "ConnectionError": self._recover_connection,
            "ValidationError": self._recover_validation_error
        }
        self.max_recovery_attempts = 3
        self.circuit_breaker_threshold = 5
        self.circuit_breaker_timeout = 60  # seconds
        self.circuit_breakers: Dict[str, Dict] = {}
        
        logger.info("RobustErrorHandler initialized with recovery strategies")
    
    def handle_error(self, error: Exception, context: Dict[str, Any] = None) -> ErrorContext:
        """Handle error with comprehensive logging and recovery attempts."""
        context = context or {}
        error_type = type(error).__name__
        
        # Create error context
        error_ctx = ErrorContext(
            error_type=error_type,
            severity=self._determine_severity(error),
            message=str(error),
            traceback=traceback.format_exc(),
            context=context,
            timestamp=time.time(),
            user_impact=self._assess_user_impact(error, context)
        )
        
        # Log error with full context
        logger.error(f"Error occurred: {error_type} - {error_ctx.message}")
        logger.debug(f"Error context: {context}")
        audit_logger.error(f"AUDIT: Error {error_type} with severity {error_ctx.severity.value}")
        
        # Check circuit breaker
        if self._is_circuit_open(error_type):
            logger.warning(f"Circuit breaker open for {error_type}, skipping recovery")
            error_ctx.user_impact = "service_degraded"
            return error_ctx
        
        # Attempt recovery if strategy exists
        if error_type in self.recovery_strategies:
            error_ctx.recovery_attempted = True
            try:
                logger.info(f"Attempting recovery for {error_type}")
                recovery_result = self.recovery_strategies[error_type](error, context)
                error_ctx.recovery_successful = recovery_result
                
                if recovery_result:
                    logger.info(f"Recovery successful for {error_type}")
                    audit_logger.info(f"AUDIT: Recovery successful for {error_type}")
                else:
                    logger.warning(f"Recovery failed for {error_type}")
                    self._update_circuit_breaker(error_type)
                    
            except Exception as recovery_error:
                logger.error(f"Recovery attempt failed: {recovery_error}")
                error_ctx.recovery_successful = False
                self._update_circuit_breaker(error_type)
        
        # Store error for analysis
        self.error_history.append(error_ctx)
        
        # Trim error history to prevent memory issues
        if len(self.error_history) > 1000:
            self.error_history = self.error_history[-500:]
        
        return error_ctx
    
    def _determine_severity(self, error: Exception) -> ErrorSeverity:
        """Determine error severity based on error type and context."""
        critical_errors = {"SystemExit", "KeyboardInterrupt", "MemoryError", "OSError"}
        high_errors = {"ImportError", "ModuleNotFoundError", "ConnectionError", "TimeoutError"}
        medium_errors = {"ValueError", "TypeError", "AttributeError", "FileNotFoundError"}
        
        error_type = type(error).__name__
        
        if error_type in critical_errors:
            return ErrorSeverity.CRITICAL
        elif error_type in high_errors:
            return ErrorSeverity.HIGH
        elif error_type in medium_errors:
            return ErrorSeverity.MEDIUM
        else:
            return ErrorSeverity.LOW
    
    def _assess_user_impact(self, error: Exception, context: Dict[str, Any]) -> str:
        """Assess the impact of an error on user experience."""
        critical_functions = context.get('critical_functions', [])
        
        if any(func in str(error) for func in critical_functions):
            return "critical"
        elif "audio_generation" in str(error).lower():
            return "feature_unavailable"
        elif "validation" in str(error).lower():
            return "input_rejected"
        else:
            return "minimal"
    
    def _is_circuit_open(self, error_type: str) -> bool:
        """Check if circuit breaker is open for given error type."""
        if error_type not in self.circuit_breakers:
            return False
            
        cb = self.circuit_breakers[error_type]
        
        # Reset circuit breaker if timeout passed
        if time.time() - cb['last_failure'] > self.circuit_breaker_timeout:
            cb['failure_count'] = 0
            cb['is_open'] = False
        
        return cb['is_open']
    
    def _update_circuit_breaker(self, error_type: str):
        """Update circuit breaker state after failure."""
        if error_type not in self.circuit_breakers:
            self.circuit_breakers[error_type] = {
                'failure_count': 0,
                'last_failure': 0,
                'is_open': False
            }
        
        cb = self.circuit_breakers[error_type]
        cb['failure_count'] += 1
        cb['last_failure'] = time.time()
        
        if cb['failure_count'] >= self.circuit_breaker_threshold:
            cb['is_open'] = True
            logger.warning(f"Circuit breaker opened for {error_type} after {cb['failure_count']} failures")
    
    def _recover_missing_dependency(self, error: Exception, context: Dict[str, Any]) -> bool:
        """Attempt to recover from missing dependency."""
        logger.info("Attempting fallback for missing dependency")
        # In production, this might try to install missing packages or use alternatives
        return True  # Mock recovery success
    
    def _recover_missing_file(self, error: Exception, context: Dict[str, Any]) -> bool:
        """Attempt to recover from missing file."""
        logger.info("Attempting to create missing directories/files")
        try:
            file_path = context.get('file_path')
            if file_path:
                Path(file_path).parent.mkdir(parents=True, exist_ok=True)
                return True
        except Exception as e:
            logger.error(f"File recovery failed: {e}")
        return False
    
    def _recover_memory_issue(self, error: Exception, context: Dict[str, Any]) -> bool:
        """Attempt to recover from memory issues."""
        logger.info("Attempting memory cleanup")
        import gc
        gc.collect()
        return True  # Mock recovery
    
    def _recover_timeout(self, error: Exception, context: Dict[str, Any]) -> bool:
        """Attempt to recover from timeout."""
        logger.info("Implementing exponential backoff for retry")
        time.sleep(1)  # Brief delay
        return True
    
    def _recover_connection(self, error: Exception, context: Dict[str, Any]) -> bool:
        """Attempt to recover from connection issues."""
        logger.info("Attempting connection recovery")
        return True  # Mock recovery
    
    def _recover_validation_error(self, error: Exception, context: Dict[str, Any]) -> bool:
        """Attempt to recover from validation errors."""
        logger.info("Applying input sanitization and validation recovery")
        return True
    
    def get_error_statistics(self) -> Dict[str, Any]:
        """Get comprehensive error statistics."""
        if not self.error_history:
            return {"total_errors": 0, "error_rate": 0.0, "recovery_rate": 0.0}
        
        total_errors = len(self.error_history)
        recovered_errors = sum(1 for e in self.error_history if e.recovery_successful)
        recent_errors = [e for e in self.error_history if time.time() - e.timestamp < 3600]  # Last hour
        
        severity_counts = {}
        for severity in ErrorSeverity:
            severity_counts[severity.value] = sum(1 for e in self.error_history if e.severity == severity)
        
        return {
            "total_errors": total_errors,
            "recent_errors": len(recent_errors),
            "recovery_rate": (recovered_errors / total_errors) * 100 if total_errors > 0 else 0,
            "error_rate": len(recent_errors) / 3600 * 100,  # Errors per hour as percentage
            "severity_distribution": severity_counts,
            "circuit_breakers": {k: v['is_open'] for k, v in self.circuit_breakers.items()}
        }


class InputValidator:
    """Comprehensive input validation and sanitization."""
    
    def __init__(self):
        self.max_prompt_length = 1000
        self.max_duration = 60.0  # seconds
        self.min_duration = 0.1
        self.allowed_file_extensions = {'.wav', '.mp3', '.flac', '.ogg', '.m4a'}
        self.dangerous_patterns = [
            r'<script', r'javascript:', r'file:///', r'../'
        ]
        self.profanity_filter_enabled = True
        
        logger.info("InputValidator initialized with comprehensive validation rules")
    
    def validate_audio_prompt(self, prompt: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Validate and sanitize audio generation prompt."""
        context = context or {}
        
        validation_result = {
            "valid": True,
            "sanitized_prompt": prompt,
            "warnings": [],
            "errors": []
        }
        
        # Length validation
        if not prompt or not prompt.strip():
            validation_result["valid"] = False
            validation_result["errors"].append("Prompt cannot be empty")
            return validation_result
        
        if len(prompt) > self.max_prompt_length:
            validation_result["warnings"].append(f"Prompt truncated to {self.max_prompt_length} characters")
            validation_result["sanitized_prompt"] = prompt[:self.max_prompt_length]
        
        # Security validation
        import re
        for pattern in self.dangerous_patterns:
            if re.search(pattern, prompt, re.IGNORECASE):
                validation_result["valid"] = False
                validation_result["errors"].append(f"Prompt contains dangerous pattern: {pattern}")
        
        # Content sanitization
        sanitized = self._sanitize_text(validation_result["sanitized_prompt"])
        if sanitized != validation_result["sanitized_prompt"]:
            validation_result["warnings"].append("Prompt was sanitized for safety")
            validation_result["sanitized_prompt"] = sanitized
        
        # Profanity filter (mock implementation)
        if self.profanity_filter_enabled and self._contains_profanity(validation_result["sanitized_prompt"]):
            validation_result["warnings"].append("Inappropriate content detected and filtered")
            validation_result["sanitized_prompt"] = self._filter_profanity(validation_result["sanitized_prompt"])
        
        logger.debug(f"Prompt validation: {validation_result['valid']}, warnings: {len(validation_result['warnings'])}")
        return validation_result
    
    def validate_audio_parameters(self, duration: float = None, temperature: float = None, 
                                 sample_rate: int = None) -> Dict[str, Any]:
        """Validate audio generation parameters."""
        validation_result = {
            "valid": True,
            "sanitized_params": {},
            "warnings": [],
            "errors": []
        }
        
        # Duration validation
        if duration is not None:
            if duration < self.min_duration:
                validation_result["warnings"].append(f"Duration increased to minimum: {self.min_duration}s")
                duration = self.min_duration
            elif duration > self.max_duration:
                validation_result["warnings"].append(f"Duration reduced to maximum: {self.max_duration}s")
                duration = self.max_duration
            validation_result["sanitized_params"]["duration"] = float(duration)
        
        # Temperature validation  
        if temperature is not None:
            if not 0.1 <= temperature <= 2.0:
                validation_result["warnings"].append("Temperature clamped to valid range (0.1-2.0)")
                temperature = max(0.1, min(2.0, temperature))
            validation_result["sanitized_params"]["temperature"] = float(temperature)
        
        # Sample rate validation
        if sample_rate is not None:
            valid_rates = [8000, 16000, 22050, 44100, 48000]
            if sample_rate not in valid_rates:
                closest_rate = min(valid_rates, key=lambda x: abs(x - sample_rate))
                validation_result["warnings"].append(f"Sample rate adjusted to nearest supported: {closest_rate}Hz")
                sample_rate = closest_rate
            validation_result["sanitized_params"]["sample_rate"] = int(sample_rate)
        
        return validation_result
    
    def validate_file_path(self, file_path: Union[str, Path]) -> Dict[str, Any]:
        """Validate file path for security and accessibility."""
        validation_result = {
            "valid": True,
            "sanitized_path": str(file_path),
            "warnings": [],
            "errors": []
        }
        
        try:
            path = Path(file_path).resolve()
            
            # Security checks
            if '..' in str(path):
                validation_result["valid"] = False
                validation_result["errors"].append("Path traversal detected")
            
            # Extension validation for audio files
            if path.suffix.lower() not in self.allowed_file_extensions:
                validation_result["warnings"].append(f"Unsupported file extension: {path.suffix}")
            
            # Path length check
            if len(str(path)) > 260:  # Windows MAX_PATH
                validation_result["valid"] = False
                validation_result["errors"].append("File path too long")
            
            validation_result["sanitized_path"] = str(path)
            
        except Exception as e:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Invalid path: {e}")
        
        return validation_result
    
    def _sanitize_text(self, text: str) -> str:
        """Sanitize text input for security."""
        # Remove HTML tags
        import re
        text = re.sub(r'<[^>]+>', '', text)
        
        # Remove control characters
        text = ''.join(char for char in text if ord(char) >= 32 or char in '\n\t')
        
        # Normalize whitespace
        text = ' '.join(text.split())
        
        return text.strip()
    
    def _contains_profanity(self, text: str) -> bool:
        """Simple profanity detection (mock implementation)."""
        # In production, this would use a proper profanity filter
        profanity_words = ['badword1', 'badword2']  # Mock list
        return any(word in text.lower() for word in profanity_words)
    
    def _filter_profanity(self, text: str) -> str:
        """Filter profanity from text (mock implementation)."""
        profanity_words = ['badword1', 'badword2']
        for word in profanity_words:
            text = text.replace(word, '*' * len(word))
        return text


class SystemHealthMonitor:
    """Comprehensive system health monitoring with alerts."""
    
    def __init__(self):
        self.metrics_history: List[HealthMetrics] = []
        self.alert_thresholds = {
            'error_rate': 10.0,  # %
            'response_time': 5.0,  # seconds
            'memory_usage': 85.0,  # %
            'cpu_usage': 80.0  # %
        }
        self.monitoring_interval = 30  # seconds
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        
        logger.info("SystemHealthMonitor initialized with alert thresholds")
    
    def start_monitoring(self):
        """Start continuous health monitoring."""
        if not self.is_monitoring:
            self.is_monitoring = True
            self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
            self.monitor_thread.start()
            logger.info("Health monitoring started")
    
    def stop_monitoring(self):
        """Stop health monitoring."""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("Health monitoring stopped")
    
    def _monitoring_loop(self):
        """Main monitoring loop running in separate thread."""
        while self.is_monitoring:
            try:
                metrics = self.collect_metrics()
                self.metrics_history.append(metrics)
                
                # Trim history to prevent memory issues
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-500:]
                
                # Check for alerts
                self._check_alerts(metrics)
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
                time.sleep(self.monitoring_interval)
    
    def collect_metrics(self) -> HealthMetrics:
        """Collect current system health metrics."""
        metrics = HealthMetrics()
        
        try:
            # Mock CPU and memory usage (in production, use psutil)
            import random
            metrics.cpu_usage = random.uniform(10, 50)  # Mock CPU usage
            metrics.memory_usage = random.uniform(30, 70)  # Mock memory usage
            
            # Calculate error rate from recent history
            if self.metrics_history:
                recent_metrics = self.metrics_history[-10:]  # Last 10 measurements
                metrics.error_rate = sum(m.error_rate for m in recent_metrics) / len(recent_metrics)
            
            # Mock response time
            metrics.response_time = random.uniform(0.1, 2.0)
            
            # Calculate success rate
            if self.metrics_history:
                recent_metrics = self.metrics_history[-5:]
                metrics.success_rate = sum(m.success_rate for m in recent_metrics) / len(recent_metrics)
            
            # Determine overall health
            metrics.system_health = self._calculate_system_health(metrics)
            
        except Exception as e:
            logger.error(f"Metrics collection error: {e}")
            metrics.system_health = SystemHealth.CRITICAL
            metrics.alerts.append(f"Metrics collection failed: {e}")
        
        return metrics
    
    def _calculate_system_health(self, metrics: HealthMetrics) -> SystemHealth:
        """Calculate overall system health based on metrics."""
        if (metrics.cpu_usage > 90 or metrics.memory_usage > 95 or 
            metrics.error_rate > 50 or metrics.response_time > 10):
            return SystemHealth.FAILURE
        elif (metrics.cpu_usage > 80 or metrics.memory_usage > 85 or 
              metrics.error_rate > 20 or metrics.response_time > 5):
            return SystemHealth.CRITICAL
        elif (metrics.cpu_usage > 60 or metrics.memory_usage > 70 or 
              metrics.error_rate > 10 or metrics.response_time > 3):
            return SystemHealth.DEGRADED
        else:
            return SystemHealth.HEALTHY
    
    def _check_alerts(self, metrics: HealthMetrics):
        """Check metrics against alert thresholds."""
        alerts = []
        
        if metrics.error_rate > self.alert_thresholds['error_rate']:
            alerts.append(f"High error rate: {metrics.error_rate:.1f}%")
        
        if metrics.response_time > self.alert_thresholds['response_time']:
            alerts.append(f"High response time: {metrics.response_time:.2f}s")
        
        if metrics.memory_usage > self.alert_thresholds['memory_usage']:
            alerts.append(f"High memory usage: {metrics.memory_usage:.1f}%")
        
        if metrics.cpu_usage > self.alert_thresholds['cpu_usage']:
            alerts.append(f"High CPU usage: {metrics.cpu_usage:.1f}%")
        
        if alerts:
            metrics.alerts.extend(alerts)
            logger.warning(f"Health alerts: {alerts}")
            audit_logger.warning(f"AUDIT: System health alerts - {alerts}")
    
    def get_health_report(self) -> Dict[str, Any]:
        """Get comprehensive health report."""
        if not self.metrics_history:
            return {"status": "no_data", "message": "No metrics collected yet"}
        
        latest = self.metrics_history[-1]
        recent_metrics = self.metrics_history[-10:] if len(self.metrics_history) >= 10 else self.metrics_history
        
        return {
            "current_status": latest.system_health.value,
            "active_alerts": latest.alerts,
            "current_metrics": {
                "cpu_usage": latest.cpu_usage,
                "memory_usage": latest.memory_usage,
                "error_rate": latest.error_rate,
                "response_time": latest.response_time,
                "success_rate": latest.success_rate
            },
            "trends": {
                "avg_cpu_usage": sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics),
                "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
                "avg_error_rate": sum(m.error_rate for m in recent_metrics) / len(recent_metrics),
                "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics)
            },
            "monitoring_duration": len(self.metrics_history) * self.monitoring_interval,
            "total_measurements": len(self.metrics_history)
        }


def robust_retry(max_retries: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """Decorator for robust retry with exponential backoff."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt < max_retries:
                        logger.warning(f"Attempt {attempt + 1} failed for {func.__name__}: {e}. Retrying in {current_delay}s...")
                        time.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logger.error(f"All {max_retries + 1} attempts failed for {func.__name__}")
            
            raise last_exception
        return wrapper
    return decorator


@contextmanager
def resource_manager(resource_name: str):
    """Context manager for proper resource cleanup."""
    logger.debug(f"Acquiring resource: {resource_name}")
    try:
        yield resource_name
    except Exception as e:
        logger.error(f"Error using resource {resource_name}: {e}")
        raise
    finally:
        logger.debug(f"Releasing resource: {resource_name}")
        # In production, perform actual cleanup


class RobustAudioDemo:
    """Enhanced Generation 2 demo with comprehensive robustness features."""
    
    def __init__(self):
        """Initialize robust demo with all safety and monitoring features."""
        self.output_dir = Path("generation2_robust_outputs")
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize robust components
        self.error_handler = RobustErrorHandler()
        self.validator = InputValidator()
        self.health_monitor = SystemHealthMonitor()
        
        # Initialize core components with error handling
        self.planner = None
        self.model = None
        self.processor = None
        
        self._initialize_components()
        
        # Performance and safety metrics
        self.metrics = {
            "tasks_executed": 0,
            "errors_handled": 0,
            "validations_performed": 0,
            "recoveries_successful": 0,
            "security_incidents": 0,
            "total_duration": 0.0
        }
        
        # Start health monitoring
        self.health_monitor.start_monitoring()
        
        logger.info("RobustAudioDemo initialized with comprehensive safety features")
        audit_logger.info("AUDIT: RobustAudioDemo session started")
    
    def _initialize_components(self):
        """Initialize core components with robust error handling."""
        try:
            self.planner = QuantumTaskPlanner(max_concurrent_tasks=2)
            logger.info("QuantumTaskPlanner initialized successfully")
        except Exception as e:
            error_ctx = self.error_handler.handle_error(e, {"component": "planner"})
            if not error_ctx.recovery_successful:
                logger.warning("Using fallback planner")
        
        try:
            if CORE_AVAILABLE:
                self.model = FugattoModel("robust-fugatto-model")
                self.processor = AudioProcessor()
                logger.info("Audio components initialized successfully")
        except Exception as e:
            error_ctx = self.error_handler.handle_error(e, {"component": "audio"})
            logger.warning("Using mock audio components")
    
    @robust_retry(max_retries=2, delay=0.5)
    def generate_audio_robust(self, prompt: str, duration: float = 5.0, **kwargs) -> Optional[Dict[str, Any]]:
        """Generate audio with comprehensive validation and error handling."""
        with resource_manager("audio_generation"):
            # Validate inputs
            prompt_validation = self.validator.validate_audio_prompt(prompt, {"function": "generate_audio"})
            self.metrics["validations_performed"] += 1
            
            if not prompt_validation["valid"]:
                logger.error(f"Prompt validation failed: {prompt_validation['errors']}")
                audit_logger.warning(f"AUDIT: Invalid prompt rejected - {prompt_validation['errors']}")
                self.metrics["security_incidents"] += 1
                return None
            
            param_validation = self.validator.validate_audio_parameters(duration=duration, **kwargs)
            if param_validation["warnings"]:
                logger.warning(f"Parameter warnings: {param_validation['warnings']}")
            
            # Use sanitized inputs
            sanitized_prompt = prompt_validation["sanitized_prompt"]
            sanitized_params = param_validation["sanitized_params"]
            duration = sanitized_params.get("duration", duration)
            
            try:
                # Generate audio with monitoring
                start_time = time.time()
                
                if self.model:
                    logger.info(f"Generating audio: '{sanitized_prompt}' ({duration}s)")
                    audio_data = self.model.generate(
                        prompt=sanitized_prompt,
                        duration_seconds=duration,
                        **sanitized_params
                    )
                else:
                    # Mock generation with validation
                    logger.info(f"Mock generating audio: '{sanitized_prompt}' ({duration}s)")
                    import numpy as np
                    sample_rate = 48000
                    num_samples = int(duration * sample_rate)
                    
                    # Generate structured mock audio based on prompt
                    t = np.linspace(0, duration, num_samples)
                    base_freq = 440.0 + (hash(sanitized_prompt) % 200)
                    audio_data = 0.3 * np.sin(2 * np.pi * base_freq * t)
                    audio_data = audio_data.astype(np.float32)
                
                generation_time = time.time() - start_time
                
                result = {
                    "audio_data": audio_data,
                    "sample_rate": 48000,
                    "duration": duration,
                    "prompt": sanitized_prompt,
                    "generation_time": generation_time,
                    "validation_warnings": prompt_validation["warnings"] + param_validation["warnings"],
                    "security_status": "validated"
                }
                
                self.metrics["tasks_executed"] += 1
                logger.info(f"Audio generation successful in {generation_time:.3f}s")
                return result
                
            except Exception as e:
                error_ctx = self.error_handler.handle_error(e, {
                    "prompt": sanitized_prompt,
                    "duration": duration,
                    "function": "generate_audio",
                    "critical_functions": ["audio_generation"]
                })
                self.metrics["errors_handled"] += 1
                
                if error_ctx.recovery_successful:
                    self.metrics["recoveries_successful"] += 1
                    logger.info("Attempting generation retry after recovery")
                    # In a real implementation, we'd retry here
                    return None
                else:
                    logger.error(f"Audio generation failed: {error_ctx.message}")
                    return None
    
    def run_robust_demo_suite(self) -> Dict[str, Any]:
        """Run comprehensive robust demo with all safety features."""
        logger.info("Starting robust demo suite")
        audit_logger.info("AUDIT: Robust demo suite initiated")
        
        demo_results = {
            "demo_version": "Generation 2 - MAKE IT ROBUST",
            "timestamp": time.time(),
            "robustness_features": [
                "comprehensive_error_handling",
                "input_validation_and_sanitization", 
                "health_monitoring_and_alerting",
                "circuit_breakers",
                "graceful_degradation",
                "audit_logging",
                "resource_management",
                "retry_mechanisms"
            ],
            "test_results": {},
            "security_summary": {},
            "performance_summary": {},
            "health_report": {}
        }
        
        # Test scenarios with different risk levels
        test_scenarios = [
            {"name": "valid_input_test", "prompt": "Beautiful piano melody", "duration": 3.0},
            {"name": "edge_case_test", "prompt": "" * 10, "duration": 0.05},  # Very short
            {"name": "security_test", "prompt": "<script>alert('xss')</script>Safe prompt", "duration": 2.0},
            {"name": "length_test", "prompt": "A" * 2000, "duration": 10.0},  # Very long
            {"name": "parameter_test", "prompt": "Normal prompt", "duration": 100.0},  # Too long duration
        ]
        
        start_time = time.time()
        
        for scenario in test_scenarios:
            logger.info(f"Running test scenario: {scenario['name']}")
            scenario_start = time.time()
            
            try:
                result = self.generate_audio_robust(
                    prompt=scenario["prompt"],
                    duration=scenario["duration"]
                )
                
                scenario_result = {
                    "status": "success" if result else "handled_gracefully",
                    "execution_time": time.time() - scenario_start,
                    "audio_generated": result is not None,
                    "validation_warnings": result.get("validation_warnings", []) if result else [],
                    "security_status": result.get("security_status", "rejected") if result else "rejected"
                }
                
                demo_results["test_results"][scenario["name"]] = scenario_result
                logger.info(f"Scenario {scenario['name']} completed: {scenario_result['status']}")
                
            except Exception as e:
                logger.error(f"Scenario {scenario['name']} failed: {e}")
                demo_results["test_results"][scenario["name"]] = {
                    "status": "failed",
                    "error": str(e),
                    "execution_time": time.time() - scenario_start
                }
                self.metrics["errors_handled"] += 1
        
        # Collect comprehensive results
        total_duration = time.time() - start_time
        self.metrics["total_duration"] = total_duration
        
        # Security summary
        demo_results["security_summary"] = {
            "validations_performed": self.metrics["validations_performed"],
            "security_incidents_blocked": self.metrics["security_incidents"],
            "input_sanitizations": sum(len(result.get("validation_warnings", [])) for result in demo_results["test_results"].values() if isinstance(result, dict)),
            "audit_entries_created": "multiple"  # Mock count
        }
        
        # Performance summary
        demo_results["performance_summary"] = {
            "total_execution_time": total_duration,
            "tasks_executed": self.metrics["tasks_executed"],
            "errors_handled": self.metrics["errors_handled"],
            "recovery_rate": (self.metrics["recoveries_successful"] / max(1, self.metrics["errors_handled"])) * 100,
            "average_response_time": total_duration / len(test_scenarios),
            "error_statistics": self.error_handler.get_error_statistics()
        }
        
        # Health report
        demo_results["health_report"] = self.health_monitor.get_health_report()
        
        # Calculate overall robustness score
        robustness_score = self._calculate_robustness_score(demo_results)
        demo_results["robustness_score"] = robustness_score
        
        # Save results
        results_file = self.output_dir / "generation2_robust_results.json"
        with open(results_file, 'w') as f:
            json.dump(demo_results, f, indent=2, default=str)
        
        logger.info(f"Robust demo suite completed. Robustness score: {robustness_score:.1f}%")
        audit_logger.info(f"AUDIT: Robust demo suite completed - Score: {robustness_score:.1f}%")
        
        return demo_results
    
    def _calculate_robustness_score(self, results: Dict[str, Any]) -> float:
        """Calculate overall robustness score based on test results."""
        scores = []
        
        # Error handling score
        if self.metrics["errors_handled"] > 0:
            error_score = (self.metrics["recoveries_successful"] / self.metrics["errors_handled"]) * 100
        else:
            error_score = 100  # No errors is perfect
        scores.append(error_score)
        
        # Validation score
        validation_score = min(100, (self.metrics["validations_performed"] / len(results["test_results"])) * 100)
        scores.append(validation_score)
        
        # Security score
        security_score = 100 if self.metrics["security_incidents"] == 0 else 90
        scores.append(security_score)
        
        # Health score
        health_report = results.get("health_report", {})
        current_status = health_report.get("current_status", "healthy")
        health_score = {
            "healthy": 100,
            "degraded": 80,
            "critical": 60,
            "failure": 30,
            "no_data": 50
        }.get(current_status, 50)
        scores.append(health_score)
        
        # Test success rate
        successful_tests = sum(1 for result in results["test_results"].values() 
                              if isinstance(result, dict) and result.get("status") in ["success", "handled_gracefully"])
        success_rate = (successful_tests / len(results["test_results"])) * 100
        scores.append(success_rate)
        
        # Calculate weighted average
        weights = [0.25, 0.15, 0.25, 0.15, 0.20]  # Error handling, validation, security, health, success rate
        robustness_score = sum(score * weight for score, weight in zip(scores, weights))
        
        return robustness_score
    
    def cleanup(self):
        """Cleanup resources and stop monitoring."""
        logger.info("Performing cleanup")
        self.health_monitor.stop_monitoring()
        audit_logger.info("AUDIT: RobustAudioDemo session ended")


def main():
    """Main robust demo execution."""
    print("üõ°Ô∏è Fugatto Audio Lab - Generation 2: MAKE IT ROBUST")
    print("=====================================================\n")
    
    demo = None
    try:
        # Initialize robust demo
        demo = RobustAudioDemo()
        
        # Run comprehensive robust demo
        results = demo.run_robust_demo_suite()
        
        # Display results
        print("\n‚ú® Robust Demo Completed Successfully!")
        print(f"üõ°Ô∏è Robustness Score: {results['robustness_score']:.1f}%")
        print(f"üìã Test Scenarios: {len(results['test_results'])}")
        print(f"üîê Security Incidents Blocked: {results['security_summary']['security_incidents_blocked']}")
        print(f"‚öôÔ∏è Validations Performed: {results['security_summary']['validations_performed']}")
        print(f"üîÑ Errors Handled: {results['performance_summary']['errors_handled']}")
        print(f"üéØ Recovery Rate: {results['performance_summary']['recovery_rate']:.1f}%")
        print(f"üìä System Health: {results['health_report'].get('current_status', 'unknown')}")
        
        if results["robustness_score"] >= 90:
            print("\nüéÜ Generation 2 Implementation: EXCELLENT")
            print("   All robustness features are working optimally!")
        elif results["robustness_score"] >= 75:
            print("\nüéâ Generation 2 Implementation: SUCCESS")
            print("   Robust features are working well with minor areas for improvement.")
        elif results["robustness_score"] >= 60:
            print("\n‚úÖ Generation 2 Implementation: ACCEPTABLE")
            print("   Basic robustness is in place but could be enhanced.")
        else:
            print("\n‚ö†Ô∏è Generation 2 Implementation: NEEDS IMPROVEMENT")
            print("   Some robustness features need attention.")
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå Demo execution failed: {e}")
        logger.error(f"Demo execution failed: {e}")
        return 1
    
    finally:
        if demo:
            demo.cleanup()


if __name__ == "__main__":
    exit(main())
