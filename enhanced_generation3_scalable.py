#!/usr/bin/env python3
"""Enhanced Generation 3 - MAKE IT SCALE (Optimized)

Adds comprehensive performance optimization and scaling capabilities:
- Asynchronous processing and concurrency
- Intelligent caching and memoization
- Dynamic resource allocation and auto-scaling
- Load balancing and distribution
- Performance profiling and optimization
- Predictive scaling and resource management
- Multi-tier caching strategies
- Connection pooling and resource reuse
"""

import asyncio
import logging
import time
import json
import threading
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from functools import wraps, lru_cache
import weakref
import queue
import multiprocessing as mp
from collections import defaultdict, deque
import hashlib
import pickle
import warnings

# Configure performance-optimized logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('generation3_scalable.log'),
        logging.FileHandler('performance.log')
    ]
)

logger = logging.getLogger(__name__)
perf_logger = logging.getLogger('performance')

# Import core components with graceful fallback
try:
    from fugatto_lab import (
        QuantumTaskPlanner, 
        QuantumTask, 
        TaskPriority,
        create_audio_generation_pipeline,
        run_quantum_audio_pipeline
    )
    from fugatto_lab.core import FugattoModel, AudioProcessor
    CORE_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Core import fallback: {e}")
    CORE_AVAILABLE = False
    
    # High-performance mock implementations
    class ScalableQuantumTaskPlanner:
        def __init__(self, max_concurrent_tasks=8):
            self.tasks = deque()
            self.max_concurrent = max_concurrent_tasks
            self.executor = ThreadPoolExecutor(max_workers=max_concurrent_tasks)
            self.active_tasks = 0
            
        async def add_task(self, task):
            self.tasks.append(task)
            return f"scalable_task_{len(self.tasks)}"
        
        async def execute_pipeline(self, pipeline_id):
            return {"status": "optimized_execution", "results": [], "tasks_executed": len(self.tasks)}
    
    QuantumTaskPlanner = ScalableQuantumTaskPlanner
    QuantumTask = dict
    TaskPriority = type("TaskPriority", (), {"HIGH": "high", "MEDIUM": "medium", "LOW": "low", "CRITICAL": "critical"})
    
    def create_audio_generation_pipeline(prompts):
        return ScalableQuantumTaskPlanner(max_concurrent_tasks=len(prompts))
    
    async def run_quantum_audio_pipeline(planner, pipeline_id=None):
        return await planner.execute_pipeline(pipeline_id)


class ScalingStrategy(Enum):
    """Different auto-scaling strategies."""
    REACTIVE = "reactive"  # Scale based on current load
    PREDICTIVE = "predictive"  # Scale based on predicted load
    AGGRESSIVE = "aggressive"  # Pre-emptive scaling
    CONSERVATIVE = "conservative"  # Minimal scaling
    HYBRID = "hybrid"  # Combination of strategies


class ResourceType(Enum):
    """Types of resources that can be scaled."""
    CPU = "cpu"
    MEMORY = "memory"
    GPU = "gpu"
    NETWORK = "network"
    STORAGE = "storage"
    THREADS = "threads"
    PROCESSES = "processes"


@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for optimization."""
    throughput: float = 0.0  # tasks/second
    latency: float = 0.0  # average response time
    cpu_utilization: float = 0.0  # %
    memory_utilization: float = 0.0  # %
    cache_hit_rate: float = 0.0  # %
    concurrent_tasks: int = 0
    queue_length: int = 0
    error_rate: float = 0.0  # %
    bandwidth_usage: float = 0.0  # MB/s
    resource_efficiency: float = 0.0  # %
    timestamp: float = field(default_factory=time.time)


@dataclass 
class ScalingDecision:
    """Auto-scaling decision with rationale."""
    resource_type: ResourceType
    action: str  # 'scale_up', 'scale_down', 'maintain'
    current_value: int
    target_value: int
    confidence: float  # 0-1
    rationale: str
    estimated_benefit: float  # expected performance gain
    timestamp: float = field(default_factory=time.time)


class IntelligentCache:
    """Multi-tier intelligent caching system with adaptive eviction."""
    
    def __init__(self, l1_size: int = 100, l2_size: int = 1000, l3_size: int = 10000):
        self.l1_cache = {}  # Hot cache (in-memory)
        self.l2_cache = {}  # Warm cache (compressed)
        self.l3_cache = {}  # Cold cache (serialized)
        
        self.l1_max_size = l1_size
        self.l2_max_size = l2_size
        self.l3_max_size = l3_size
        
        # Access tracking for intelligent eviction
        self.access_counts = defaultdict(int)
        self.access_times = defaultdict(float)
        self.cache_sizes = defaultdict(int)
        
        # Performance metrics
        self.hits = {"l1": 0, "l2": 0, "l3": 0}
        self.misses = 0
        
        logger.info(f"IntelligentCache initialized: L1={l1_size}, L2={l2_size}, L3={l3_size}")
    
    def _generate_key(self, *args, **kwargs) -> str:
        """Generate cache key from arguments."""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.sha256(key_data.encode()).hexdigest()[:32]
    
    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache with intelligent tier management."""
        self.access_counts[key] += 1
        self.access_times[key] = time.time()
        
        # Check L1 (hot cache)
        if key in self.l1_cache:
            self.hits["l1"] += 1
            perf_logger.debug(f"L1 cache hit: {key}")
            return self.l1_cache[key]
        
        # Check L2 (warm cache)
        if key in self.l2_cache:
            self.hits["l2"] += 1
            value = self.l2_cache[key]
            # Promote to L1
            await self._promote_to_l1(key, value)
            perf_logger.debug(f"L2 cache hit: {key} (promoted to L1)")
            return value
        
        # Check L3 (cold cache)
        if key in self.l3_cache:
            self.hits["l3"] += 1
            try:
                value = pickle.loads(self.l3_cache[key])
                # Promote to L2
                await self._promote_to_l2(key, value)
                perf_logger.debug(f"L3 cache hit: {key} (promoted to L2)")
                return value
            except Exception as e:
                logger.error(f"L3 cache deserialization error: {e}")
                del self.l3_cache[key]
        
        # Cache miss
        self.misses += 1
        perf_logger.debug(f"Cache miss: {key}")
        return None
    
    async def set(self, key: str, value: Any, tier: int = 1) -> None:
        """Set value in cache with intelligent placement."""
        self.access_counts[key] = 1
        self.access_times[key] = time.time()
        
        # Calculate value size for cache management
        try:
            value_size = len(pickle.dumps(value))
            self.cache_sizes[key] = value_size
        except:
            value_size = 1000  # Default size estimate
        
        if tier == 1 or self.access_counts[key] > 10:
            await self._set_l1(key, value)
        elif tier == 2 or self.access_counts[key] > 3:
            await self._set_l2(key, value)
        else:
            await self._set_l3(key, value)
    
    async def _set_l1(self, key: str, value: Any) -> None:
        """Set value in L1 cache with intelligent eviction."""
        # Evict if necessary
        while len(self.l1_cache) >= self.l1_max_size:
            await self._evict_from_l1()
        
        self.l1_cache[key] = value
        perf_logger.debug(f"Set L1 cache: {key}")
    
    async def _set_l2(self, key: str, value: Any) -> None:
        """Set value in L2 cache."""
        while len(self.l2_cache) >= self.l2_max_size:
            await self._evict_from_l2()
        
        self.l2_cache[key] = value
        perf_logger.debug(f"Set L2 cache: {key}")
    
    async def _set_l3(self, key: str, value: Any) -> None:
        """Set value in L3 cache (serialized)."""
        while len(self.l3_cache) >= self.l3_max_size:
            await self._evict_from_l3()
        
        try:
            self.l3_cache[key] = pickle.dumps(value)
            perf_logger.debug(f"Set L3 cache: {key}")
        except Exception as e:
            logger.error(f"L3 cache serialization error: {e}")
    
    async def _promote_to_l1(self, key: str, value: Any) -> None:
        """Promote value from L2 to L1."""
        if key in self.l2_cache:
            del self.l2_cache[key]
        await self._set_l1(key, value)
    
    async def _promote_to_l2(self, key: str, value: Any) -> None:
        """Promote value from L3 to L2."""
        if key in self.l3_cache:
            del self.l3_cache[key]
        await self._set_l2(key, value)
    
    async def _evict_from_l1(self) -> None:
        """Evict least valuable item from L1 using hybrid strategy."""
        if not self.l1_cache:
            return
        
        # Score items by access frequency and recency
        current_time = time.time()
        scores = {}
        
        for key in self.l1_cache:
            access_count = self.access_counts[key]
            last_access = self.access_times[key]
            recency_score = 1.0 / (1.0 + current_time - last_access)
            frequency_score = access_count / 100.0
            
            # Combined score with recency bias
            scores[key] = (frequency_score * 0.6) + (recency_score * 0.4)
        
        # Evict lowest scoring item
        victim_key = min(scores.keys(), key=lambda k: scores[k])
        victim_value = self.l1_cache.pop(victim_key)
        
        # Demote to L2
        await self._set_l2(victim_key, victim_value)
        perf_logger.debug(f"Evicted from L1: {victim_key} (demoted to L2)")
    
    async def _evict_from_l2(self) -> None:
        """Evict from L2 cache."""
        if not self.l2_cache:
            return
        
        # Use LRU for L2
        victim_key = min(self.l2_cache.keys(), key=lambda k: self.access_times[k])
        victim_value = self.l2_cache.pop(victim_key)
        
        # Demote to L3
        await self._set_l3(victim_key, victim_value)
        perf_logger.debug(f"Evicted from L2: {victim_key} (demoted to L3)")
    
    async def _evict_from_l3(self) -> None:
        """Evict from L3 cache (permanent removal)."""
        if not self.l3_cache:
            return
        
        victim_key = min(self.l3_cache.keys(), key=lambda k: self.access_times[k])
        del self.l3_cache[victim_key]
        
        # Clean up tracking data
        if victim_key in self.access_counts:
            del self.access_counts[victim_key]
        if victim_key in self.access_times:
            del self.access_times[victim_key]
        if victim_key in self.cache_sizes:
            del self.cache_sizes[victim_key]
        
        perf_logger.debug(f"Evicted from L3: {victim_key} (permanently removed)")
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive cache performance metrics."""
        total_hits = sum(self.hits.values())
        total_requests = total_hits + self.misses
        
        return {
            "hit_rate": (total_hits / max(1, total_requests)) * 100,
            "miss_rate": (self.misses / max(1, total_requests)) * 100,
            "l1_hit_rate": (self.hits["l1"] / max(1, total_requests)) * 100,
            "l2_hit_rate": (self.hits["l2"] / max(1, total_requests)) * 100,
            "l3_hit_rate": (self.hits["l3"] / max(1, total_requests)) * 100,
            "l1_size": len(self.l1_cache),
            "l2_size": len(self.l2_cache),
            "l3_size": len(self.l3_cache),
            "total_requests": total_requests,
            "cache_efficiency": self._calculate_efficiency()
        }
    
    def _calculate_efficiency(self) -> float:
        """Calculate cache efficiency score."""
        metrics = {
            "l1_ratio": len(self.l1_cache) / max(1, self.l1_max_size),
            "l2_ratio": len(self.l2_cache) / max(1, self.l2_max_size),
            "l3_ratio": len(self.l3_cache) / max(1, self.l3_max_size)
        }
        
        total_hits = sum(self.hits.values())
        total_requests = total_hits + self.misses
        hit_rate = total_hits / max(1, total_requests)
        
        # Weighted efficiency considering hit rates and utilization
        efficiency = (hit_rate * 0.7) + (sum(metrics.values()) / 3 * 0.3)
        return min(100, efficiency * 100)


class AutoScaler:
    """Intelligent auto-scaling system with predictive capabilities."""
    
    def __init__(self, strategy: ScalingStrategy = ScalingStrategy.HYBRID):
        self.strategy = strategy
        self.metrics_history: deque = deque(maxlen=1000)
        self.scaling_decisions: List[ScalingDecision] = []
        self.resource_limits = {
            ResourceType.THREADS: {"min": 2, "max": 32, "current": 4},
            ResourceType.PROCESSES: {"min": 1, "max": 8, "current": 2},
            ResourceType.MEMORY: {"min": 512, "max": 8192, "current": 1024},  # MB
            ResourceType.CPU: {"min": 1, "max": 16, "current": 2}  # cores
        }
        self.scaling_cooldown = 60  # seconds
        self.last_scaling_time = 0
        
        logger.info(f"AutoScaler initialized with strategy: {strategy.value}")
    
    def record_metrics(self, metrics: PerformanceMetrics) -> None:
        """Record performance metrics for scaling decisions."""
        self.metrics_history.append(metrics)
        perf_logger.info(f"Metrics recorded: throughput={metrics.throughput:.2f}, latency={metrics.latency:.3f}s")
    
    async def evaluate_scaling(self) -> List[ScalingDecision]:
        """Evaluate and make scaling decisions based on current metrics."""
        if len(self.metrics_history) < 3:
            return []  # Need minimum data for decisions
        
        if time.time() - self.last_scaling_time < self.scaling_cooldown:
            return []  # Still in cooldown period
        
        decisions = []
        current_metrics = self.metrics_history[-1]
        
        # Evaluate each resource type
        for resource_type in ResourceType:
            decision = await self._evaluate_resource(resource_type, current_metrics)
            if decision and decision.action != 'maintain':
                decisions.append(decision)
        
        # Apply scaling decisions
        for decision in decisions:
            await self._apply_scaling_decision(decision)
        
        if decisions:
            self.last_scaling_time = time.time()
            logger.info(f"Applied {len(decisions)} scaling decisions")
        
        return decisions
    
    async def _evaluate_resource(self, resource_type: ResourceType, metrics: PerformanceMetrics) -> Optional[ScalingDecision]:
        """Evaluate scaling need for specific resource type."""
        if resource_type not in self.resource_limits:
            return None
        
        limits = self.resource_limits[resource_type]
        current = limits["current"]
        min_val = limits["min"]
        max_val = limits["max"]
        
        # Calculate utilization and trends
        utilization = self._calculate_utilization(resource_type, metrics)
        trend = self._calculate_trend(resource_type)
        demand_prediction = self._predict_demand(resource_type)
        
        # Scaling decision logic based on strategy
        if self.strategy == ScalingStrategy.REACTIVE:
            action, target, confidence, rationale = await self._reactive_scaling(
                resource_type, utilization, current, min_val, max_val
            )
        elif self.strategy == ScalingStrategy.PREDICTIVE:
            action, target, confidence, rationale = await self._predictive_scaling(
                resource_type, demand_prediction, current, min_val, max_val
            )
        elif self.strategy == ScalingStrategy.AGGRESSIVE:
            action, target, confidence, rationale = await self._aggressive_scaling(
                resource_type, utilization, trend, current, min_val, max_val
            )
        elif self.strategy == ScalingStrategy.CONSERVATIVE:
            action, target, confidence, rationale = await self._conservative_scaling(
                resource_type, utilization, current, min_val, max_val
            )
        else:  # HYBRID
            action, target, confidence, rationale = await self._hybrid_scaling(
                resource_type, utilization, trend, demand_prediction, current, min_val, max_val
            )
        
        if action == 'maintain':
            return None
        
        estimated_benefit = self._estimate_benefit(resource_type, current, target, metrics)
        
        return ScalingDecision(
            resource_type=resource_type,
            action=action,
            current_value=current,
            target_value=target,
            confidence=confidence,
            rationale=rationale,
            estimated_benefit=estimated_benefit
        )
    
    def _calculate_utilization(self, resource_type: ResourceType, metrics: PerformanceMetrics) -> float:
        """Calculate current utilization for resource type."""
        if resource_type == ResourceType.CPU:
            return metrics.cpu_utilization
        elif resource_type == ResourceType.MEMORY:
            return metrics.memory_utilization
        elif resource_type == ResourceType.THREADS:
            return min(100, (metrics.concurrent_tasks / self.resource_limits[resource_type]["current"]) * 100)
        elif resource_type == ResourceType.PROCESSES:
            return min(100, (metrics.queue_length / 10) * 100)  # Arbitrary scale
        else:
            return 50.0  # Default moderate utilization
    
    def _calculate_trend(self, resource_type: ResourceType) -> float:
        """Calculate utilization trend (-1 to 1, negative = decreasing)."""
        if len(self.metrics_history) < 5:
            return 0.0
        
        recent_utilizations = []
        for metrics in list(self.metrics_history)[-5:]:
            utilization = self._calculate_utilization(resource_type, metrics)
            recent_utilizations.append(utilization)
        
        # Simple trend calculation
        if len(recent_utilizations) >= 2:
            trend = (recent_utilizations[-1] - recent_utilizations[0]) / len(recent_utilizations)
            return max(-1.0, min(1.0, trend / 20.0))  # Normalize to -1 to 1
        
        return 0.0
    
    def _predict_demand(self, resource_type: ResourceType) -> float:
        """Predict future demand using simple exponential smoothing."""
        if len(self.metrics_history) < 3:
            return 50.0  # Default prediction
        
        # Get recent utilizations
        recent_utilizations = []
        for metrics in list(self.metrics_history)[-10:]:
            utilization = self._calculate_utilization(resource_type, metrics)
            recent_utilizations.append(utilization)
        
        # Simple exponential smoothing
        alpha = 0.3
        prediction = recent_utilizations[0]
        
        for utilization in recent_utilizations[1:]:
            prediction = alpha * utilization + (1 - alpha) * prediction
        
        return prediction
    
    async def _reactive_scaling(self, resource_type: ResourceType, utilization: float, 
                               current: int, min_val: int, max_val: int) -> tuple:
        """Reactive scaling based on current utilization."""
        if utilization > 80 and current < max_val:
            target = min(max_val, current * 2)
            return 'scale_up', target, 0.8, f"High utilization: {utilization:.1f}%"
        elif utilization < 20 and current > min_val:
            target = max(min_val, current // 2)
            return 'scale_down', target, 0.7, f"Low utilization: {utilization:.1f}%"
        else:
            return 'maintain', current, 1.0, f"Utilization within bounds: {utilization:.1f}%"
    
    async def _predictive_scaling(self, resource_type: ResourceType, prediction: float,
                                 current: int, min_val: int, max_val: int) -> tuple:
        """Predictive scaling based on demand forecast."""
        if prediction > 75 and current < max_val:
            target = min(max_val, int(current * 1.5))
            return 'scale_up', target, 0.6, f"Predicted demand: {prediction:.1f}%"
        elif prediction < 25 and current > min_val:
            target = max(min_val, int(current * 0.7))
            return 'scale_down', target, 0.5, f"Predicted low demand: {prediction:.1f}%"
        else:
            return 'maintain', current, 0.8, f"Predicted demand stable: {prediction:.1f}%"
    
    async def _aggressive_scaling(self, resource_type: ResourceType, utilization: float, 
                                 trend: float, current: int, min_val: int, max_val: int) -> tuple:
        """Aggressive scaling with trend consideration."""
        if (utilization > 60 or trend > 0.2) and current < max_val:
            target = min(max_val, int(current * 1.8))
            return 'scale_up', target, 0.7, f"Aggressive scale up: util={utilization:.1f}%, trend={trend:.2f}"
        elif utilization < 30 and trend < -0.1 and current > min_val:
            target = max(min_val, int(current * 0.6))
            return 'scale_down', target, 0.6, f"Aggressive scale down: util={utilization:.1f}%, trend={trend:.2f}"
        else:
            return 'maintain', current, 0.8, "No aggressive action needed"
    
    async def _conservative_scaling(self, resource_type: ResourceType, utilization: float,
                                   current: int, min_val: int, max_val: int) -> tuple:
        """Conservative scaling with high thresholds."""
        if utilization > 90 and current < max_val:
            target = min(max_val, current + 1)
            return 'scale_up', target, 0.9, f"Conservative scale up: {utilization:.1f}%"
        elif utilization < 10 and current > min_val:
            target = max(min_val, current - 1)
            return 'scale_down', target, 0.8, f"Conservative scale down: {utilization:.1f}%"
        else:
            return 'maintain', current, 1.0, f"Conservative maintain: {utilization:.1f}%"
    
    async def _hybrid_scaling(self, resource_type: ResourceType, utilization: float, 
                             trend: float, prediction: float, current: int, 
                             min_val: int, max_val: int) -> tuple:
        """Hybrid scaling combining multiple strategies."""
        # Weighted decision based on multiple factors
        scale_up_score = 0
        scale_down_score = 0
        
        # Utilization factor
        if utilization > 75:
            scale_up_score += 3
        elif utilization < 25:
            scale_down_score += 2
        
        # Trend factor
        if trend > 0.3:
            scale_up_score += 2
        elif trend < -0.3:
            scale_down_score += 2
        
        # Prediction factor
        if prediction > 70:
            scale_up_score += 1
        elif prediction < 30:
            scale_down_score += 1
        
        # Make decision
        if scale_up_score >= 3 and current < max_val:
            target = min(max_val, int(current * 1.5))
            confidence = min(0.9, scale_up_score / 6.0)
            return 'scale_up', target, confidence, f"Hybrid scale up (score={scale_up_score})"
        elif scale_down_score >= 3 and current > min_val:
            target = max(min_val, int(current * 0.7))
            confidence = min(0.8, scale_down_score / 5.0)
            return 'scale_down', target, confidence, f"Hybrid scale down (score={scale_down_score})"
        else:
            return 'maintain', current, 0.9, "Hybrid maintain"
    
    def _estimate_benefit(self, resource_type: ResourceType, current: int, target: int, 
                         metrics: PerformanceMetrics) -> float:
        """Estimate performance benefit of scaling action."""
        if current == target:
            return 0.0
        
        scale_factor = target / current
        
        # Estimate benefit based on resource type and current bottlenecks
        if resource_type == ResourceType.THREADS and metrics.concurrent_tasks > current * 0.8:
            return (scale_factor - 1) * metrics.throughput * 0.8
        elif resource_type == ResourceType.CPU and metrics.cpu_utilization > 80:
            return (scale_factor - 1) * metrics.throughput * 0.6
        elif resource_type == ResourceType.MEMORY and metrics.memory_utilization > 85:
            return (scale_factor - 1) * metrics.throughput * 0.4
        else:
            return (scale_factor - 1) * metrics.throughput * 0.2
    
    async def _apply_scaling_decision(self, decision: ScalingDecision) -> None:
        """Apply scaling decision to resource limits."""
        if decision.resource_type in self.resource_limits:
            self.resource_limits[decision.resource_type]["current"] = decision.target_value
            self.scaling_decisions.append(decision)
            logger.info(f"Applied scaling: {decision.resource_type.value} {decision.action} to {decision.target_value}")
            perf_logger.info(f"Scaling applied: {decision.resource_type.value}={decision.target_value} (benefit: {decision.estimated_benefit:.2f})")


class AsyncTaskExecutor:
    """High-performance async task execution with load balancing."""
    
    def __init__(self, max_workers: int = 8, enable_load_balancing: bool = True):
        self.max_workers = max_workers
        self.enable_load_balancing = enable_load_balancing
        self.thread_executor = ThreadPoolExecutor(max_workers=max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=min(max_workers, mp.cpu_count()))
        
        # Load balancing
        self.worker_loads = defaultdict(int)
        self.task_queue = asyncio.Queue(maxsize=1000)
        self.results = {}
        self.active_tasks = 0
        
        # Performance tracking
        self.execution_times = deque(maxlen=1000)
        self.task_counts = {"completed": 0, "failed": 0, "pending": 0}
        
        logger.info(f"AsyncTaskExecutor initialized: {max_workers} workers, load_balancing={enable_load_balancing}")
    
    async def execute_batch(self, tasks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Execute batch of tasks with intelligent distribution."""
        start_time = time.time()
        results = {"completed": [], "failed": [], "execution_time": 0}
        
        # Categorize tasks by type and complexity
        categorized_tasks = self._categorize_tasks(tasks)
        
        # Execute tasks in parallel with load balancing
        async_tasks = []
        for task in tasks:
            async_task = asyncio.create_task(self._execute_single_task(task))
            async_tasks.append(async_task)
        
        # Wait for all tasks with progress tracking
        completed_tasks = 0
        for coro in asyncio.as_completed(async_tasks):
            try:
                result = await coro
                if result.get("status") == "success":
                    results["completed"].append(result)
                    self.task_counts["completed"] += 1
                else:
                    results["failed"].append(result)
                    self.task_counts["failed"] += 1
                
                completed_tasks += 1
                if completed_tasks % 10 == 0:
                    perf_logger.info(f"Batch progress: {completed_tasks}/{len(tasks)} tasks completed")
                    
            except Exception as e:
                logger.error(f"Task execution error: {e}")
                results["failed"].append({"error": str(e), "status": "failed"})
                self.task_counts["failed"] += 1
        
        execution_time = time.time() - start_time
        results["execution_time"] = execution_time
        results["throughput"] = len(tasks) / execution_time
        
        self.execution_times.append(execution_time)
        
        logger.info(f"Batch execution completed: {len(results['completed'])} successful, {len(results['failed'])} failed")
        perf_logger.info(f"Batch throughput: {results['throughput']:.2f} tasks/sec")
        
        return results
    
    def _categorize_tasks(self, tasks: List[Dict[str, Any]]) -> Dict[str, List]:
        """Categorize tasks by type and estimated complexity."""
        categories = {"light": [], "medium": [], "heavy": []}
        
        for task in tasks:
            complexity = self._estimate_task_complexity(task)
            if complexity < 0.3:
                categories["light"].append(task)
            elif complexity < 0.7:
                categories["medium"].append(task)
            else:
                categories["heavy"].append(task)
        
        logger.debug(f"Task categorization: light={len(categories['light'])}, medium={len(categories['medium'])}, heavy={len(categories['heavy'])}")
        return categories
    
    def _estimate_task_complexity(self, task: Dict[str, Any]) -> float:
        """Estimate task complexity (0-1 scale)."""
        complexity = 0.1
        
        # Factor in duration if specified
        if "duration" in task:
            complexity += min(0.4, task["duration"] / 30.0)  # Up to 30 seconds = max complexity from duration
        
        # Factor in prompt length
        if "prompt" in task:
            complexity += min(0.3, len(task["prompt"]) / 1000.0)  # Up to 1000 chars = max complexity from length
        
        # Factor in task type
        task_type = task.get("type", "simple")
        type_complexity = {
            "simple": 0.1,
            "audio_generation": 0.6,
            "audio_processing": 0.7,
            "batch_processing": 0.9
        }
        complexity += type_complexity.get(task_type, 0.3)
        
        return min(1.0, complexity)
    
    async def _execute_single_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Execute single task with performance monitoring."""
        task_id = task.get("id", f"task_{int(time.time() * 1000000) % 1000000}")
        start_time = time.time()
        
        try:
            # Choose execution method based on task type
            task_type = task.get("type", "simple")
            
            if task_type in ["audio_generation", "audio_processing"]:
                # CPU/GPU intensive tasks - use process executor
                result = await self._execute_compute_intensive_task(task)
            else:
                # I/O or simple tasks - use thread executor
                result = await self._execute_io_task(task)
            
            execution_time = time.time() - start_time
            
            return {
                "task_id": task_id,
                "status": "success",
                "result": result,
                "execution_time": execution_time,
                "worker_type": "process" if task_type in ["audio_generation", "audio_processing"] else "thread"
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Task {task_id} failed: {e}")
            
            return {
                "task_id": task_id,
                "status": "failed",
                "error": str(e),
                "execution_time": execution_time
            }
    
    async def _execute_compute_intensive_task(self, task: Dict[str, Any]) -> Any:
        """Execute compute-intensive task using process pool."""
        loop = asyncio.get_event_loop()
        
        # Mock compute-intensive audio processing
        def compute_task(task_data):
            import time
            import numpy as np
            
            duration = task_data.get("duration", 5.0)
            prompt = task_data.get("prompt", "default")
            
            # Simulate intensive computation
            time.sleep(min(0.1, duration / 50.0))  # Scale with duration
            
            # Generate mock audio data
            sample_rate = 48000
            num_samples = int(duration * sample_rate)
            t = np.linspace(0, duration, num_samples)
            base_freq = 440.0 + (hash(prompt) % 200)
            audio_data = 0.3 * np.sin(2 * np.pi * base_freq * t)
            
            return {
                "audio_data_shape": audio_data.shape,
                "sample_rate": sample_rate,
                "duration": duration,
                "prompt_hash": hash(prompt),
                "computation_type": "process_pool"
            }
        
        result = await loop.run_in_executor(self.process_executor, compute_task, task)
        return result
    
    async def _execute_io_task(self, task: Dict[str, Any]) -> Any:
        """Execute I/O task using thread pool."""
        loop = asyncio.get_event_loop()
        
        def io_task(task_data):
            import time
            
            # Simulate I/O operation
            time.sleep(0.01)  # Brief I/O simulation
            
            return {
                "task_type": task_data.get("type", "simple"),
                "processed_at": time.time(),
                "computation_type": "thread_pool"
            }
        
        result = await loop.run_in_executor(self.thread_executor, io_task, task)
        return result
    
    def get_performance_metrics(self) -> PerformanceMetrics:
        """Get current performance metrics."""
        avg_execution_time = sum(self.execution_times) / max(1, len(self.execution_times))
        throughput = len(self.execution_times) / max(1, sum(self.execution_times))
        
        total_tasks = sum(self.task_counts.values())
        error_rate = (self.task_counts["failed"] / max(1, total_tasks)) * 100
        
        return PerformanceMetrics(
            throughput=throughput,
            latency=avg_execution_time,
            cpu_utilization=50.0,  # Mock
            memory_utilization=40.0,  # Mock
            concurrent_tasks=self.active_tasks,
            queue_length=self.task_queue.qsize() if hasattr(self.task_queue, 'qsize') else 0,
            error_rate=error_rate,
            resource_efficiency=75.0  # Mock
        )
    
    def shutdown(self):
        """Graceful shutdown of executors."""
        logger.info("Shutting down task executors")
        self.thread_executor.shutdown(wait=True)
        self.process_executor.shutdown(wait=True)


class ScalableAudioDemo:
    """Generation 3 demo showcasing comprehensive scalability and performance optimization."""
    
    def __init__(self):
        """Initialize scalable demo with performance optimization components."""
        self.output_dir = Path("generation3_scalable_outputs")
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize scalability components
        self.cache = IntelligentCache(l1_size=50, l2_size=200, l3_size=1000)
        self.auto_scaler = AutoScaler(strategy=ScalingStrategy.HYBRID)
        self.executor = AsyncTaskExecutor(max_workers=8)
        
        # Initialize core components
        self.planner = None
        self.model = None
        self.processor = None
        
        self._initialize_components()
        
        # Performance tracking
        self.metrics = {
            "tasks_executed": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "scaling_decisions": 0,
            "concurrent_peak": 0,
            "total_duration": 0.0,
            "throughput_peak": 0.0
        }
        
        # Start performance monitoring
        self.monitoring_task = None
        
        logger.info("ScalableAudioDemo initialized with comprehensive performance optimization")
    
    def _initialize_components(self):
        """Initialize components with error handling."""
        try:
            self.planner = QuantumTaskPlanner(max_concurrent_tasks=8)
            logger.info("Scalable QuantumTaskPlanner initialized")
        except Exception as e:
            logger.warning(f"Planner initialization failed: {e}")
        
        try:
            if CORE_AVAILABLE:
                self.model = FugattoModel("scalable-fugatto-model")
                self.processor = AudioProcessor()
                logger.info("Scalable audio components initialized")
        except Exception as e:
            logger.warning(f"Audio components initialization failed: {e}")
    
    async def start_monitoring(self):
        """Start performance monitoring loop."""
        self.monitoring_task = asyncio.create_task(self._monitoring_loop())
        logger.info("Performance monitoring started")
    
    async def _monitoring_loop(self):
        """Continuous performance monitoring and auto-scaling."""
        while True:
            try:
                # Collect performance metrics
                metrics = self.executor.get_performance_metrics()
                self.auto_scaler.record_metrics(metrics)
                
                # Evaluate scaling decisions
                scaling_decisions = await self.auto_scaler.evaluate_scaling()
                self.metrics["scaling_decisions"] += len(scaling_decisions)
                
                # Update peak metrics
                self.metrics["concurrent_peak"] = max(self.metrics["concurrent_peak"], metrics.concurrent_tasks)
                self.metrics["throughput_peak"] = max(self.metrics["throughput_peak"], metrics.throughput)
                
                # Log performance summary
                perf_logger.info(f"Performance snapshot: throughput={metrics.throughput:.2f}, latency={metrics.latency:.3f}s, concurrent={metrics.concurrent_tasks}")
                
                await asyncio.sleep(10)  # Monitor every 10 seconds
                
            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
                await asyncio.sleep(30)
    
    async def generate_audio_scalable(self, prompt: str, duration: float = 5.0, use_cache: bool = True) -> Optional[Dict[str, Any]]:
        """Generate audio with intelligent caching and scaling."""
        # Generate cache key
        cache_key = self.cache._generate_key(prompt, duration)
        
        # Check cache first
        if use_cache:
            cached_result = await self.cache.get(cache_key)
            if cached_result:
                self.metrics["cache_hits"] += 1
                logger.debug(f"Cache hit for prompt: {prompt[:50]}...")
                return cached_result
            else:
                self.metrics["cache_misses"] += 1
        
        # Generate audio
        start_time = time.time()
        
        try:
            if self.model:
                # Real audio generation
                logger.info(f"Generating scalable audio: '{prompt}' ({duration}s)")
                audio_data = self.model.generate(
                    prompt=prompt,
                    duration_seconds=duration,
                    temperature=0.8
                )
            else:
                # High-performance mock generation
                logger.info(f"Mock generating scalable audio: '{prompt}' ({duration}s)")
                import numpy as np
                
                sample_rate = 48000
                num_samples = int(duration * sample_rate)
                t = np.linspace(0, duration, num_samples)
                base_freq = 440.0 + (hash(prompt) % 200)
                
                # Optimized vectorized generation
                audio_data = 0.3 * np.sin(2 * np.pi * base_freq * t)
                
                # Add harmonic complexity
                for harmonic in [2, 3]:
                    audio_data += 0.1 * np.sin(2 * np.pi * base_freq * harmonic * t)
                
                audio_data = audio_data.astype(np.float32)
            
            generation_time = time.time() - start_time
            
            result = {
                "audio_data": audio_data,
                "sample_rate": 48000,
                "duration": duration,
                "prompt": prompt,
                "generation_time": generation_time,
                "cached": False,
                "optimization_level": "scalable"
            }
            
            # Cache the result
            if use_cache:
                await self.cache.set(cache_key, result)
            
            self.metrics["tasks_executed"] += 1
            logger.info(f"Scalable audio generation completed in {generation_time:.3f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"Scalable audio generation failed: {e}")
            return None
    
    async def run_scalability_benchmark(self) -> Dict[str, Any]:
        """Run comprehensive scalability and performance benchmark."""
        logger.info("Starting scalability benchmark suite")
        
        benchmark_results = {
            "demo_version": "Generation 3 - MAKE IT SCALE",
            "timestamp": time.time(),
            "scalability_features": [
                "intelligent_multi_tier_caching",
                "adaptive_auto_scaling",
                "concurrent_task_execution",
                "load_balancing",
                "performance_profiling",
                "resource_optimization",
                "predictive_scaling",
                "async_processing"
            ],
            "benchmark_results": {},
            "cache_performance": {},
            "scaling_performance": {},
            "overall_metrics": {}
        }
        
        # Start monitoring
        await self.start_monitoring()
        
        # Benchmark scenarios
        scenarios = [
            {"name": "small_batch", "tasks": 5, "concurrent": 2, "duration": 2.0},
            {"name": "medium_batch", "tasks": 20, "concurrent": 5, "duration": 3.0},
            {"name": "large_batch", "tasks": 50, "concurrent": 10, "duration": 2.5},
            {"name": "stress_test", "tasks": 100, "concurrent": 15, "duration": 1.5},
            {"name": "cache_test", "tasks": 30, "concurrent": 8, "duration": 3.0, "repeat_prompts": True}
        ]
        
        total_start_time = time.time()
        
        for scenario in scenarios:
            logger.info(f"Running scalability scenario: {scenario['name']}")
            scenario_start = time.time()
            
            # Generate test tasks
            tasks = self._generate_benchmark_tasks(
                count=scenario["tasks"],
                duration=scenario["duration"],
                repeat_prompts=scenario.get("repeat_prompts", False)
            )
            
            # Execute tasks with concurrency control
            try:
                # Create semaphore for concurrency control
                semaphore = asyncio.Semaphore(scenario["concurrent"])
                
                async def execute_with_semaphore(task):
                    async with semaphore:
                        return await self.generate_audio_scalable(
                            prompt=task["prompt"],
                            duration=task["duration"]
                        )
                
                # Execute all tasks
                task_results = []
                async_tasks = [execute_with_semaphore(task) for task in tasks]
                
                for coro in asyncio.as_completed(async_tasks):
                    result = await coro
                    if result:
                        task_results.append(result)
                
                scenario_duration = time.time() - scenario_start
                
                # Analyze results
                successful_tasks = len([r for r in task_results if r])
                total_generation_time = sum(r.get("generation_time", 0) for r in task_results if r)
                avg_generation_time = total_generation_time / max(1, successful_tasks)
                throughput = successful_tasks / scenario_duration
                cached_results = len([r for r in task_results if r and r.get("cached", False)])
                
                scenario_result = {
                    "tasks_requested": scenario["tasks"],
                    "tasks_completed": successful_tasks,
                    "success_rate": (successful_tasks / scenario["tasks"]) * 100,
                    "total_duration": scenario_duration,
                    "avg_generation_time": avg_generation_time,
                    "throughput": throughput,
                    "cache_hits": cached_results,
                    "cache_hit_rate": (cached_results / max(1, successful_tasks)) * 100,
                    "concurrent_limit": scenario["concurrent"]
                }
                
                benchmark_results["benchmark_results"][scenario["name"]] = scenario_result
                
                logger.info(f"Scenario {scenario['name']} completed: {successful_tasks}/{scenario['tasks']} tasks, {throughput:.2f} tasks/sec")
                
                # Brief pause between scenarios
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Scenario {scenario['name']} failed: {e}")
                benchmark_results["benchmark_results"][scenario["name"]] = {
                    "error": str(e),
                    "status": "failed"
                }
        
        # Collect final metrics
        total_duration = time.time() - total_start_time
        self.metrics["total_duration"] = total_duration
        
        # Cache performance metrics
        cache_metrics = self.cache.get_metrics()
        benchmark_results["cache_performance"] = cache_metrics
        
        # Scaling performance metrics
        benchmark_results["scaling_performance"] = {
            "scaling_decisions_made": self.metrics["scaling_decisions"],
            "concurrent_peak": self.metrics["concurrent_peak"],
            "throughput_peak": self.metrics["throughput_peak"],
            "auto_scaling_strategy": self.auto_scaler.strategy.value
        }
        
        # Overall performance metrics
        total_tasks = sum(
            result.get("tasks_completed", 0) 
            for result in benchmark_results["benchmark_results"].values()
            if isinstance(result, dict) and "tasks_completed" in result
        )
        
        benchmark_results["overall_metrics"] = {
            "total_execution_time": total_duration,
            "total_tasks_executed": total_tasks,
            "overall_throughput": total_tasks / total_duration,
            "cache_efficiency": cache_metrics.get("hit_rate", 0),
            "resource_utilization": 75.0,  # Mock
            "scalability_score": self._calculate_scalability_score(benchmark_results)
        }
        
        # Stop monitoring
        if self.monitoring_task:
            self.monitoring_task.cancel()
        
        # Save results
        results_file = self.output_dir / "generation3_scalability_results.json"
        with open(results_file, 'w') as f:
            json.dump(benchmark_results, f, indent=2, default=str)
        
        logger.info(f"Scalability benchmark completed. Score: {benchmark_results['overall_metrics']['scalability_score']:.1f}%")
        
        return benchmark_results
    
    def _generate_benchmark_tasks(self, count: int, duration: float, repeat_prompts: bool = False) -> List[Dict[str, Any]]:
        """Generate benchmark tasks with variety."""
        base_prompts = [
            "Peaceful ocean waves",
            "Gentle piano melody",
            "Birds singing in forest",
            "Soft rain on leaves",
            "Warm jazz saxophone",
            "Mountain stream flowing",
            "Evening campfire crackling",
            "Classical string quartet",
            "Acoustic guitar strumming",
            "Thunder in distance"
        ]
        
        tasks = []
        for i in range(count):
            if repeat_prompts and i >= len(base_prompts):
                # Repeat prompts to test caching
                prompt = base_prompts[i % len(base_prompts)]
            else:
                prompt = base_prompts[i % len(base_prompts)] + f" variation {i // len(base_prompts) + 1}"
            
            tasks.append({
                "id": f"benchmark_task_{i+1}",
                "type": "audio_generation",
                "prompt": prompt,
                "duration": duration + (i % 3) * 0.5,  # Vary duration slightly
            })
        
        return tasks
    
    def _calculate_scalability_score(self, results: Dict[str, Any]) -> float:
        """Calculate comprehensive scalability score."""
        scores = []
        
        # Throughput score
        overall_throughput = results["overall_metrics"]["overall_throughput"]
        throughput_score = min(100, overall_throughput * 10)  # 10 tasks/sec = 100%
        scores.append(throughput_score)
        
        # Cache efficiency score
        cache_efficiency = results["cache_performance"].get("hit_rate", 0)
        scores.append(cache_efficiency)
        
        # Success rate score
        success_rates = []
        for result in results["benchmark_results"].values():
            if isinstance(result, dict) and "success_rate" in result:
                success_rates.append(result["success_rate"])
        avg_success_rate = sum(success_rates) / max(1, len(success_rates))
        scores.append(avg_success_rate)
        
        # Scaling effectiveness score
        scaling_decisions = results["scaling_performance"]["scaling_decisions_made"]
        scaling_score = min(100, 50 + scaling_decisions * 10)  # Base 50% + bonus for adaptability
        scores.append(scaling_score)
        
        # Concurrency handling score
        concurrent_peak = results["scaling_performance"]["concurrent_peak"]
        concurrency_score = min(100, concurrent_peak * 10)  # 10 concurrent = 100%
        scores.append(concurrency_score)
        
        # Calculate weighted average
        weights = [0.3, 0.2, 0.2, 0.15, 0.15]  # Throughput, cache, success, scaling, concurrency
        scalability_score = sum(score * weight for score, weight in zip(scores, weights))
        
        return scalability_score
    
    def cleanup(self):
        """Cleanup resources."""
        logger.info("Cleaning up scalable demo resources")
        if self.monitoring_task and not self.monitoring_task.done():
            self.monitoring_task.cancel()
        self.executor.shutdown()


def performance_profiler(func):
    """Decorator for detailed performance profiling."""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = 0  # Mock memory measurement
        
        try:
            result = await func(*args, **kwargs)
            
            execution_time = time.time() - start_time
            memory_used = 0  # Mock memory calculation
            
            perf_logger.info(f"Performance: {func.__name__} - Time: {execution_time:.3f}s, Memory: {memory_used}MB")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            perf_logger.error(f"Performance: {func.__name__} - Failed in {execution_time:.3f}s - {e}")
            raise
    
    return wrapper


async def main():
    """Main scalable demo execution."""
    print(" Fugatto Audio Lab - Generation 3: MAKE IT SCALE")
    print("=================================================\n")
    
    demo = None
    try:
        # Initialize scalable demo
        demo = ScalableAudioDemo()
        
        # Run comprehensive scalability benchmark
        results = await demo.run_scalability_benchmark()
        
        # Display results
        print("\n Scalability Benchmark Completed Successfully!")
        print(f" Scalability Score: {results['overall_metrics']['scalability_score']:.1f}%")
        print(f" Overall Throughput: {results['overall_metrics']['overall_throughput']:.2f} tasks/sec")
        print(f" Cache Hit Rate: {results['cache_performance']['hit_rate']:.1f}%")
        print(f" Auto-Scaling Decisions: {results['scaling_performance']['scaling_decisions_made']}")
        print(f" Peak Concurrent Tasks: {results['scaling_performance']['concurrent_peak']}")
        print(f"  Total Execution Time: {results['overall_metrics']['total_execution_time']:.2f}s")
        print(f" Tasks Completed: {results['overall_metrics']['total_tasks_executed']}")
        
        # Performance breakdown by scenario
        print("\n Scenario Performance:")
        for scenario_name, scenario_result in results["benchmark_results"].items():
            if isinstance(scenario_result, dict) and "throughput" in scenario_result:
                print(f"  {scenario_name}: {scenario_result['throughput']:.2f} tasks/sec ({scenario_result['success_rate']:.1f}% success)")
        
        if results["overall_metrics"]["scalability_score"] >= 90:
            print("\n Generation 3 Implementation: EXCEPTIONAL")
            print("   Scalability and performance optimization are outstanding!")
        elif results["overall_metrics"]["scalability_score"] >= 75:
            print("\n Generation 3 Implementation: EXCELLENT")
            print("   Strong scalability with high performance optimization.")
        elif results["overall_metrics"]["scalability_score"] >= 60:
            print("\n Generation 3 Implementation: SUCCESS")
            print("   Good scalability with effective performance features.")
        else:
            print("\n Generation 3 Implementation: NEEDS OPTIMIZATION")
            print("   Basic scalability present but performance could be improved.")
        
        return 0
        
    except Exception as e:
        print(f"\n Scalability benchmark failed: {e}")
        logger.error(f"Scalability benchmark failed: {e}")
        return 1
    
    finally:
        if demo:
            demo.cleanup()


if __name__ == "__main__":
    exit(asyncio.run(main()))
